往 MQ 里写入的数据是非常核、及关键的。绝对不容许有丢失，如果⼀旦 MQ 中间件故障，那么这个系统⽴⻢就会把核⼼数据写⼊本地磁盘⽂件。
但是如果说在⾼峰期并发量⽐较⾼的情况下，接收到⼀条数据⽴⻢同步写本地磁盘⽂件，这个性能绝对是极其差的。
我们的核⼼思路是⼀旦 MQ 中间件故障触发降级，不是⽴⻢写本地磁盘，⽽是采⽤内存双缓冲 + 批量刷磁盘的机制。
系统接收到⼀条消息就会⽴⻢写内存缓冲，然后开启⼀个后台线程把内存缓冲的数据刷新到磁盘上去。

如果投递出去的消息在⽹络传输过程中丢失，或者在 RabbitMQ 的内存中还没写⼊磁盘的时候宕机，都会导致⽣产端投递到 MQ 的数据丢失。
⽽且丢失之后⽣产端⾃⼰还感知不到，同时还没办法来补救。所以⽣产端需要开启⼀个 confirm 模式，接收投递到 MQ的消息，
如果 MQ ⼀旦将消息持久化到磁盘之后，必须也要回传⼀个 confirm 消息给⽣产端。

开辟两块内存空间，也就是经典的内存双缓冲机制。
然后核⼼数据进来全部写第⼀块缓冲区，写满了之后，由⼀个线程进⾏那块缓冲区的数据批量
刷到磁盘⽂件的⼯作，其他线程同时可以继续写另外⼀块缓冲区。
⼀块缓冲区刷磁盘的同时，另外⼀块缓冲区可以接受其他线程的写⼊。
针对多线程写内存时需加锁，但是直接加一个锁必然导致所有的线程都是串行，会大大降低性能。
因此在这⾥必须要对内存双缓冲机制引⼊分段加锁机制，也就是将内存缓冲切分为多个分⽚，
每个内存缓冲分⽚就对应⼀个锁：
DoubleBuffer doubleBufferSlice = doubleBuffer.chooseRandom();//从一个内存缓冲分片集合中选一个自动负载均衡
synchronized(doubleBufferSlice){
	doubleBufferSlice.write(data);
	if(doubleBufferSlice.isFull()){
		doubleBufferSlice.exchange();//缓存区1写满了则让缓存区2提供写服务
	}
}
doubleBufferSlice.flush();//将缓冲区的数据刷入内存(若放入同步代码块则写缓存区1满会继续持锁执行刷磁盘操作导致其他线程写缓存阻塞)


-----------------------------------------------------------

使⽤缓存集群的时候，最怕的就是热 key、⼤ value 这两种情况，热 key就是你的缓存集群中的某个 key 瞬间被数万甚⾄⼗万的并发请求打爆。
⼤ value就是你的某个 key 对应的 value 可能有 GB 级的⼤⼩，导致查询 value 的时候导致⽹络相关的故障问题。 以至于使用集群部署的
Redis也会因为单个从节点扛不住崩掉，大量的数据再通过查数据库写缓存到另一个从节点而继续击溃他，最后导致整个集群崩掉。
所以，此时完全可以基于⼤数据领域的流式计算技术来进⾏实时数据访问次数的统计，⽐如发现⼀秒之内，某条数据突然访问次数超过
了 1000，就直接⽴⻢把这条数据判定为是热点数据，可以将这个发现出来的热点数据写⼊⽐如 zookeeper 中。我们⾃⼰的系统可以对
zookeeper 指定的热点缓存对应的 znode 进⾏监听，如果有变化他⽴⻢就可以感知到了。
在每个系统内部其实还应该专⻔加⼀个对热点数据访问的限流熔断保护措施，每个系统实例每秒最多请求缓存集群读操作不超过多少次，
超过就可以熔断掉不让请求缓存集群，直接返回⼀个空⽩信息，然后⽤户稍后会⾃⾏再次重新刷新⻚⾯之类的。
通过系统层⾃⼰直接加限流熔断保护措施，可以很好的保护后⾯的缓存集群、数据库集群。

Eureka使用ConcurrentHashMap 将维护注册表、拉取注册表、更新⼼跳时间，全部发⽣在内存⾥！这是Eureka Server ⾮常核⼼的⼀个点。

-----------------------------------------------------------

RabbitMQ：他的好处在于可以⽀撑⾼并发、⾼吞吐、性能很⾼，同时有⾮常完善便捷的后台管理界⾯可以使⽤
	⽀持集群化、⾼可⽤部署架构、消息⾼可靠⽀持，功能较为完善，基于 erlang 语⾔开发需要较为扎实的 erlang 语⾔功底才可以定制化
RocketMQ：是阿⾥开源的，经过阿里的生产环境的超⾼并发、⾼吞吐的考验，性能卓越，同时还⽀持分布式事务等特殊场景。是基于 Java 语⾔开发的，适合深⼊阅读源码
Kafka：优势在于专为超⾼吞吐量的实时⽇志采集、实时数据同步、实时数据计算

采⽤ MQ 中间件来实现系统解耦、异步调⽤、流量削峰，但是会带来系统可用性下降、消息一致性问题
RabbitMQ的消息的持久化：
	RabbitMQ⼀旦宕机就再次重启，就会丢失我们之前创建的queue，所以⾸先得先让queue是持久化的。
	还有⼀个重要的点，在你发送消息到RabbitMQ的时候，需要定义这条消息也是持久化的。
		这⾥要注意⼀点，RabbitMQ的消息持久化，是不承诺100%的消息不丢失的。
		因为有可能RabbitMQ接收到了消息在内存，但是还没来得及持久化到磁盘，他⾃⼰就宕机了，这个时候消息还是会丢失的。
		如果要完全100%保证写⼊RabbitMQ的数据必须落地磁盘，不会丢失，需要依靠其他的机制
			消费者⼿动 ack机制保证消息不丢失，必须要消费者确保⾃⼰处理完毕了⼀个消息，
			才能⼿动发送ack 给 MQ，MQ 收到 ack 之后才会删除这个消息。
			RabbitMQ 投递消息的时候，都是会带上本次消息投递的⼀个 delivery tag 唯⼀标识⼀次消息投递。

			// false关闭autoACK
			channel.basicConsume(
				QUEUE_NAME， false， deliverCallback， consumerTag ->{}
			);
			DeliverCallback deliverCallback = (consumerTag， delivery) -> {
				try{
				// 发送消息
				} cahth {
					// 返回nack：true批量
					channeldelivery.basicAck(delivery.getEnvelope().getDeliveryTag()，true);
				} finally {
					// 返回ack：true尝试投递给其他消费者，false 的话会导致 RabbitMQ 知道你处理失败，但是还是删除这条消息
					channeldelivery.basicNack(delivery.getEnvelope().getDeliveryTag()，true);
				}
			}
			那么RabbitMQ 他能够⽆限制的不停给你的消费者服务实例推送uack消息吗?
			如果 RabbitMQ 给你的消费者服务实例推送的消息过多过快，那么此时这⼏千条消息都是 unack 的状态，
			⼀直积压着有可能会导致消费者服务实例的内存溢出。RabbitMQ 基于⼀个 prefetch count	来控制
			unack message 的数量。你可以通过 “channel.basicQos(10)” 设置当前 channel 的 prefetch count。
			⽐如你要是设置为 10 的话，那么意味着当前这个 channel ⾥，unack message 的数量不能超过 10 个，
			以此来避免消费者服务实例积压 unack message 过多。
			RabbitMQ 官⽅给出的建议是 prefetch count ⼀般设置在 100~300之间，其实在我们的实践中，
			这个 prefetch count ⼤家完全是可以⾃⼰去压测⼀下的。
	集群化部署 + 数据多副本冗余
	必须得让⽣产者通过⼀些参数的设置，保证写⼀条消息到某台机器，必须同步这条消息到另外⼀台机器，
	集群⾥有双副本了，然后此时才可以认为这条消息写成功了。
	你的⽣产者同理应该基于参数设置⼀下，集群⾥必须有超过 2 台机器可以接收你的数据副本复制。

-----------------------------------------------------------

Kafka 是⾼吞吐低延迟的⾼并发、⾼性能的消息中间件，在⼤数据领域有极为⼴泛的运⽤。
写入内存--->刷入磁盘，以磁盘顺序写入末尾追加效率高
零拷⻉--->读磁盘然后放入内存中(零拷⻉技术直接将数据发送到网卡)


zookeeper特性：
顺序一致性：同一个客户端的请求严格按照其发起的顺序执行
原子性：事务的一次执行在每个server上应该是一致的，要么都执行，要么都不执行
单一视图：每个server的数据视图都是一样的
实时性：在一定时间内，zookeeper应该保证client读取的数据是最新的

leader服务器选取规则：
优先检查zxid，zxid大的作为leader服务器
zxid相同就比较myid大小，myid大的作为leader服务器
只有获取过半server的支持才能成为leader